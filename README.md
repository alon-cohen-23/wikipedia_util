# wikipedia_uril
## Prepare training data
All dependencies for preparing the datasets is stored in dataset/requirements.txt

After installation, make sure to run the following Python code from a Python terminal *once* before running:
```
import nltk
nltk.download('punkt')
```
### wikipedia
TODO
### Text files
We assume that the input is a collection of text files. See dataset/full_flow.py for documentation. 
## Train and Evaluate Model
### wandb
Create user.
When running, put the API key in the WANDB_API_KEY environment variable
## Translate
TODO